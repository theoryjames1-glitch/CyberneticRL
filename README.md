# ğŸ§­ Cybernetic Reinforcement Learning (CyberneticRL)

This repository collects **theories, formalisms, and code** for **Cybernetic Reinforcement Learning** â€”  
a framework where agents regulate themselves using **internal cybernetic signals** in addition to external rewards.  

Traditional RL maximizes expected return from the environment.  
**CyberneticRL generalizes this** by introducing a *cybernetic reward factor* that reflects the agentâ€™s own viability (stability, plasticity, diversity, attractor robustness).  

---

## ğŸ“– Core Idea

Instead of relying only on environment reward:

\[
J(\pi) = \mathbb{E}\Big[\sum_t \gamma^t r_t^{\text{env}}\Big]
\]

CyberneticRL uses a **cybernetic reward multiplier**:

\[
r_t^{\text{total}} = r_t^{\text{env}} \cdot R_t^{\text{cyber}}
\]

where \(R_t^{\text{cyber}}\) is derived from internal signals (grad norms, entropy, contraction, etc.).

---

## ğŸ”‘ Cybernetic Signals

- **R_task** â†’ Task reward (external environment or metric).  
- **R_stab** â†’ Stability (bounded gradients, spectral norms).  
- **R_div** â†’ Diversity (attention entropy, behavioral variety).  
- **R_plast** â†’ Plasticity (healthy adaptation, non-frozen learning).  
- **R_attr** â†’ Attractor fit (robust hidden state convergence).  
- *(Optional)* **R_bci** â†’ Brain-computer or human feedback.  

These signals are combined via **weighted sum**, **fuzzy logic**, or **adaptive weighting**.

---

## âš™ï¸ Adaptation Law

Parameters evolve as:

\[
\theta_{t+1} = \theta_t + \alpha \,\nabla_\theta \big( r_t^{\text{env}} \cdot R_t^{\text{cyber}} \big)
\]

The cybernetic reward acts as a **homeostatic regulator**:  
- Suppressing updates when unstable.  
- Amplifying updates when task-aligned & healthy.  

---

## ğŸŒ€ Dual Feedback Loops

- **Outer loop (task)** â†’ Agent interacts with environment â†’ gets \(r_t^{\text{env}}\).  
- **Inner loop (cybernetic)** â†’ Agent monitors itself â†’ computes \(R_t^{\text{cyber}}\).  

This forms a **cybernetic control system** for learning.

---

## ğŸ§® Example Code Snippets

### Cybernetic Reward Computer

```python
reward_comp = CyberneticRewardComputer()

signals = {
    "R_task": np.exp(-loss.item()),
    "R_stab": stability_monitor(model),
    "R_div": diversity_monitor(outputs),
    "R_plast": plasticity_monitor(model),
    "R_attr": attractor_monitor(outputs)
}

cyber_reward = reward_comp.compute(signals)[0]
cyber_loss = loss * cyber_reward
````

---

### CyberneticRL Loop (pseudo-code)

```python
for episode in range(num_episodes):
    state = env.reset()
    for t in range(max_steps):
        action = policy(state)
        next_state, reward_env, done, _ = env.step(action)

        # internal cybernetic signals
        signals = monitor.compute(model, loss, inputs, outputs)

        # combine with environment reward
        cyber_reward = reward_comp.compute(signals)[0]
        total_reward = reward_env * cyber_reward

        # RL update
        update_policy(total_reward, state, action, next_state)

        state = next_state
        if done:
            break
```

---

## ğŸ”¬ Experiments

* **Compare** CyberneticRL vs standard RL.
* Track signals (`R_stab`, `R_div`, etc.) over time.
* Ablations: run with subsets of signals.
* Plot phase transitions (when cyber signals cross thresholds).

---

## ğŸ“‚ Repo Structure

```
CyberneticRL-Theory/
â”‚
â”œâ”€â”€ docs/                  # extended theory pages
â”‚   â”œâ”€â”€ theory_cyberneticrl.md
â”‚   â”œâ”€â”€ cybernetic_signals.md
â”‚   â”œâ”€â”€ fuzzy_logic.md
â”‚   â”œâ”€â”€ bci_integration.md
â”‚   â””â”€â”€ experiments.md
â”‚
â”œâ”€â”€ examples/              # runnable prototypes
â”‚   â”œâ”€â”€ cybernetic_reward.py
â”‚   â”œâ”€â”€ cybernetic_monitor.py
â”‚   â””â”€â”€ cybernetic_rl_loop.py
â”‚
â””â”€â”€ README.md              # this file
```

---

## ğŸ“Œ Roadmap

* [x] Core theory of CyberneticRL
* [x] Signal monitors (stability, diversity, plasticity, attractors)
* [x] Cybernetic reward computer
* [ ] Fuzzy logic integration
* [ ] BCI feedback integration
* [ ] Full RL benchmark experiments

---

## ğŸ“š References

* Cybernetics: W. Ross Ashby, *Design for a Brain* (1952).
* Control Theory & RL: Sutton & Barto, *Reinforcement Learning* (1998).
* Recent works on stability in transformers, entropy regularization, RLHF.

---

âœ¨ *CyberneticRL = RL with self-awareness. The agent doesnâ€™t just chase rewards â€” it regulates itself like a living system.*
